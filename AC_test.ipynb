{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from actor_critic import Agent\n",
    "from RL.rl_env_2 import bending_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 3\n",
    "n_actions = 10\n",
    "# Some fixed parameter\n",
    "strip_length = 40\n",
    "pre_length = 0.1\n",
    "k = 0.05\n",
    "N_episodes = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test1_ep1001\n",
      "gen_abaqus_model_step.py Success\n",
      "abaqus脚本执行成功\n",
      "Successfully generated stress collection script for test1_ep1001\n",
      "Stress collection success\n",
      "data cleaning finished\n",
      "Action is 7\n",
      "Idx is 1469\n",
      "Parameter is [321.05536016324794, 7.6892068365515245, 0.0, 0, -0.0, 0.02510354787096952]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Optimizing_bending_parameter\\actor_critic.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_abaqus_model_step.py Success\n",
      "abaqus脚本执行成功\n",
      "Successfully generated stress collection script for test1_ep1001\n",
      "Stress collection success\n",
      "data cleaning finished\n",
      "Action is 7\n",
      "Idx is 1960\n",
      "Parameter is [320.99811719964725, 10.230702100884875, 0.0, 0, -0.0, 0.03396629870443017]\n",
      "   0\n",
      "0  7\n",
      "[[321.1, 0.0, 0.0, 0.0, -0.0, 0.0], [321.05536016324794, 7.6892068365515245, 0.0, 0, -0.0, 0.02510354787096952]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Optimizing_bending_parameter\\actor_critic.py:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen_spring_back_model.py Success\n",
      "Successfully generated springback collection script for test1_ep1001\n",
      "Springback collection success\n",
      "data cleaning finished\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mchoose_action(observation)\n\u001b[0;32m     12\u001b[0m next_state, _, _, _, reward, done, springback \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m---> 13\u001b[0m agent\u001b[39m.\u001b[39;49mlearn(observation, next_state, reward, done)\n\u001b[0;32m     14\u001b[0m cum_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n\u001b[0;32m     15\u001b[0m observation \u001b[39m=\u001b[39m next_state\n",
      "File \u001b[1;32mc:\\Optimizing_bending_parameter\\actor_critic.py:78\u001b[0m, in \u001b[0;36mAgent.learn\u001b[1;34m(self, state, new_state, reward, done)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39m# print(value_1)\u001b[39;00m\n\u001b[0;32m     76\u001b[0m value_2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mforward(new_state)\n\u001b[1;32m---> 78\u001b[0m reward \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(reward)\n\u001b[0;32m     80\u001b[0m delta \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m value_2 \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39m\u001b[39mint\u001b[39m(done)) \u001b[39m-\u001b[39m value_1\n\u001b[0;32m     82\u001b[0m critic_loss \u001b[39m=\u001b[39m delta \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "agent = Agent(input_dim=input_dim, n_actions=n_actions)\n",
    "env = bending_env(episode=1000)\n",
    "score_history = []\n",
    "reward_history = []\n",
    "\n",
    "for i in range(N_episodes):\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        next_state, _, _, _, reward, done, springback = env.step(action)\n",
    "        if done:\n",
    "            reward = springback * 100\n",
    "        agent.learn(observation, next_state, reward, done)\n",
    "        cum_reward += reward\n",
    "        observation = next_state\n",
    "    score_history.append(springback)\n",
    "    reward_history.append(cum_reward)\n",
    "    print(\"Cumulated reward is {}\".format(cum_reward))\n",
    "    print(\"Episode {} score is {}\".format(i, springback))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"Score history:\")\n",
    "print(score_history)\n",
    "print(\"-\" * 50)\n",
    "print(\"Reward history\")\n",
    "print(reward_history)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.19 ('bending_parameter')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb5682a02b22986d5da7cf5ad9a87df9e6aaf685707959f756f6733e237bfd4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
